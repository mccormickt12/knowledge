\documentclass[landscape]{article}
%ss[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage[shortlabels]{enumitem}
\usepackage{enumerate}



\pdfinfo{
/Title (knowledge.pdf)
/Creator (TeX)
/Producer (pdfTeX 1.40.0)
/Author (Tom McCormick, Skyler Rojas)
/Keywords (pdflatex, latex,pdftex,tex)}

% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.3in,left=.3in,right=.3in,bottom=.3in} }
    {\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
    }

% Turn off header and footer
\pagestyle{empty}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                            {-1ex plus -.5ex minus -.2ex}%
                            {0.5ex plus .2ex}%x
                            {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                            {-1explus -.5ex minus -.2ex}%
                            {0.5ex plus .2ex}%
                            {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                            {-1ex plus -.5ex minus -.2ex}%
                            {1ex plus .2ex}%
                            {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

%My Environments
\newtheorem{example}[section]{Example}
% -----------------------------------------------------------------------

\def\ci{\perp\!\!\!\perp}



\begin{document}
\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
    \Large{\underline{Knowledge Sheet}} \\
\end{center}

\section*{Subjects}

\subsection*{Probability}
Given: $T(n) = a \times T(\frac{n}{b}) + O(n^d)$

\begin{description}

\item[a)]
$O(n^d)$ if $d > log_b(a)$
\end{description}


\subsection*{Linear Algebra}


\subsection*{Sorting Algorithm Runtimes}
$HeapSort \textit{ and } MergeSort \rightarrow \Theta{(n\log{n})}$

$InsertionSort \rightarrow \Omega{(n)} \textit{ to } O(n^2)$

$QuickSort \rightarrow \Omega{(n\log{n})} \textit{ to } O(n^2)$

\subsection*{Security}

\subsection*{Unix}

\subsection*{Graph Algorithms}


\subsection{Machine Learning}

\textbf{Classification}
Let $\mathcal{Y}$ denote a set of classes. For a boolean classification, such as spam filters, $\mathcal{Y} = \{0, 1\}$. Let the observation $x \in \mathbb{R}^d$ denote a $d$-dimensional vector of features.

\textbf{K-Nearest Neighbors: $\Theta(n(d + \log k))$}

Let $x_i \ldots x_{i+k}$ be the $k$ nearest observations to $x$ from the trained set. Classify $x$ with the class with the plurality of votes from the $k$ feature vectors.

Note: for efficiency, normalize feature vectors to have the same range of values before applying k-NN. Algorithm performs worse if the dimension $d$ is too large.

\textbf{Random Forests}
A $RandomForest$ is a collection of decision trees. The goal is to reduce the error rate by classifying an observation $x$ through many decision trees and selecting the most common class as the result. Assumes each decision tree is different. Can create random  trees through either

1) Bagging (used more): train each tree on a random subset of the training set $S$. If $S$ contains $n$ test observations, then sample with replacement $n$ times to create our random subset $S^* \in S$. The expected size of $S^*$ is about 63\% the size of $S$. Use the Decision Tree Inference algorithm to construct a tree from $S^*$.

2) Feature Sampling: using a subset of $d$ features when constructing the tree. Reduces the likely hood that each tree has the same prominent feature at its root.


\section*{Code}

\rule{0.3\linewidth}{0.25pt}
\newpage
\scriptsize
\bibliographystyle{abstract}
\bibliography{refFile}
\end{multicols}
\end{document}